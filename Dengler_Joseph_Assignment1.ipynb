{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_19 (Dense)             (None, 256)               200960    \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 10)                2570      \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 269,322\n",
      "Trainable params: 269,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/30\n",
      "48000/48000 [==============================] - 10s 218us/step - loss: 0.9583 - accuracy: 0.7689 - val_loss: 0.4254 - val_accuracy: 0.8933\n",
      "Epoch 2/30\n",
      "48000/48000 [==============================] - 10s 210us/step - loss: 0.3911 - accuracy: 0.8918 - val_loss: 0.3207 - val_accuracy: 0.9102\n",
      "Epoch 3/30\n",
      "48000/48000 [==============================] - 10s 199us/step - loss: 0.3216 - accuracy: 0.9084 - val_loss: 0.2808 - val_accuracy: 0.9208\n",
      "Epoch 4/30\n",
      "48000/48000 [==============================] - 9s 192us/step - loss: 0.2855 - accuracy: 0.9186 - val_loss: 0.2567 - val_accuracy: 0.9289\n",
      "Epoch 5/30\n",
      "48000/48000 [==============================] - 9s 192us/step - loss: 0.2599 - accuracy: 0.9257 - val_loss: 0.2384 - val_accuracy: 0.9334\n",
      "Epoch 6/30\n",
      "48000/48000 [==============================] - 9s 193us/step - loss: 0.2398 - accuracy: 0.9321 - val_loss: 0.2228 - val_accuracy: 0.9371\n",
      "Epoch 7/30\n",
      "48000/48000 [==============================] - 7s 144us/step - loss: 0.2224 - accuracy: 0.9374 - val_loss: 0.2093 - val_accuracy: 0.9411\n",
      "Epoch 8/30\n",
      "48000/48000 [==============================] - 6s 129us/step - loss: 0.2075 - accuracy: 0.9413 - val_loss: 0.1980 - val_accuracy: 0.9457\n",
      "Epoch 9/30\n",
      "48000/48000 [==============================] - 6s 126us/step - loss: 0.1951 - accuracy: 0.9442 - val_loss: 0.1867 - val_accuracy: 0.9491\n",
      "Epoch 10/30\n",
      "48000/48000 [==============================] - 6s 135us/step - loss: 0.1836 - accuracy: 0.9480 - val_loss: 0.1800 - val_accuracy: 0.9504\n",
      "Epoch 11/30\n",
      "48000/48000 [==============================] - 6s 133us/step - loss: 0.1730 - accuracy: 0.9510 - val_loss: 0.1705 - val_accuracy: 0.9530\n",
      "Epoch 12/30\n",
      "48000/48000 [==============================] - 7s 149us/step - loss: 0.1636 - accuracy: 0.9532 - val_loss: 0.1637 - val_accuracy: 0.9539\n",
      "Epoch 13/30\n",
      "48000/48000 [==============================] - 7s 137us/step - loss: 0.1552 - accuracy: 0.9559 - val_loss: 0.1626 - val_accuracy: 0.9539\n",
      "Epoch 14/30\n",
      "48000/48000 [==============================] - 6s 135us/step - loss: 0.1477 - accuracy: 0.9586 - val_loss: 0.1531 - val_accuracy: 0.9574\n",
      "Epoch 15/30\n",
      "48000/48000 [==============================] - 7s 136us/step - loss: 0.1404 - accuracy: 0.9604 - val_loss: 0.1456 - val_accuracy: 0.9586\n",
      "Epoch 16/30\n",
      "48000/48000 [==============================] - 7s 136us/step - loss: 0.1340 - accuracy: 0.9622 - val_loss: 0.1419 - val_accuracy: 0.9597\n",
      "Epoch 17/30\n",
      "48000/48000 [==============================] - 7s 143us/step - loss: 0.1278 - accuracy: 0.9642 - val_loss: 0.1364 - val_accuracy: 0.9614\n",
      "Epoch 18/30\n",
      "48000/48000 [==============================] - 7s 141us/step - loss: 0.1221 - accuracy: 0.9657 - val_loss: 0.1335 - val_accuracy: 0.9628\n",
      "Epoch 19/30\n",
      "48000/48000 [==============================] - 7s 156us/step - loss: 0.1168 - accuracy: 0.9672 - val_loss: 0.1303 - val_accuracy: 0.9630\n",
      "Epoch 20/30\n",
      "48000/48000 [==============================] - 7s 156us/step - loss: 0.1122 - accuracy: 0.9690 - val_loss: 0.1263 - val_accuracy: 0.9652\n",
      "Epoch 21/30\n",
      "48000/48000 [==============================] - 7s 149us/step - loss: 0.1076 - accuracy: 0.9699 - val_loss: 0.1237 - val_accuracy: 0.9643\n",
      "Epoch 22/30\n",
      "48000/48000 [==============================] - 8s 163us/step - loss: 0.1032 - accuracy: 0.9714 - val_loss: 0.1211 - val_accuracy: 0.9657\n",
      "Epoch 23/30\n",
      "48000/48000 [==============================] - 7s 145us/step - loss: 0.0993 - accuracy: 0.9724 - val_loss: 0.1183 - val_accuracy: 0.9658\n",
      "Epoch 24/30\n",
      "48000/48000 [==============================] - 7s 148us/step - loss: 0.0955 - accuracy: 0.9741 - val_loss: 0.1175 - val_accuracy: 0.9663\n",
      "Epoch 25/30\n",
      "48000/48000 [==============================] - 7s 149us/step - loss: 0.0919 - accuracy: 0.9749 - val_loss: 0.1136 - val_accuracy: 0.9675\n",
      "Epoch 26/30\n",
      "48000/48000 [==============================] - 7s 141us/step - loss: 0.0885 - accuracy: 0.9757 - val_loss: 0.1109 - val_accuracy: 0.9682\n",
      "Epoch 27/30\n",
      "48000/48000 [==============================] - 7s 140us/step - loss: 0.0851 - accuracy: 0.9767 - val_loss: 0.1091 - val_accuracy: 0.9684\n",
      "Epoch 28/30\n",
      "48000/48000 [==============================] - 5s 110us/step - loss: 0.0821 - accuracy: 0.9781 - val_loss: 0.1090 - val_accuracy: 0.9683\n",
      "Epoch 29/30\n",
      "48000/48000 [==============================] - 5s 110us/step - loss: 0.0793 - accuracy: 0.9788 - val_loss: 0.1056 - val_accuracy: 0.9698\n",
      "Epoch 30/30\n",
      "48000/48000 [==============================] - 6s 118us/step - loss: 0.0767 - accuracy: 0.9793 - val_loss: 0.1047 - val_accuracy: 0.9695\n",
      "10000/10000 [==============================] - 1s 51us/step\n",
      "\n",
      "Test score: 0.10025386319495738\n",
      "Test accuracy: 0.9696999788284302\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "np.random.seed(1671)  # for reproducibility\n",
    "\n",
    "# Network and training\n",
    "NB_EPOCH = 30 # Left at 30 for improved accuracy on tests moving forward\n",
    "BATCH_SIZE = 64 # Left at 64 for improved accuracy on tests moving forward\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10  # number of outputs = number of digits\n",
    "OPTIMIZER = SGD()  # optimizer, explained later in this chapter\n",
    "N_HIDDEN = 256\n",
    "VALIDATION_SPLIT = 0.2  # how much TRAIN is reserved for VALIDATION\n",
    "RESHAPED = 784\n",
    "\n",
    "# Data: shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# Normalize\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=OPTIMIZER,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
    "                    verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "# Evaluate the model\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"\\nTest score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Results: Modifying Epochs, Batch Size, and Hidden Neurons\n",
    "\n",
    "## Experiment 1: Modifying Epochs (Default Batch Size: 128)\n",
    "\n",
    "I first experimented with different epoch values to see how the number of epochs impacts the training, validation, and test accuracy of the model. I tested 10, 20, and 30 epochs with the default batch size of 128.\n",
    "\n",
    "### Results:\n",
    "\n",
    "- **Epochs: 10**\n",
    "  - Training Accuracy: 92.44%\n",
    "  - Validation Accuracy: 92.81%\n",
    "  - Test Accuracy: 92.61%\n",
    "\n",
    "- **Epochs: 20**\n",
    "  - Training Accuracy: 94.56%\n",
    "  - Validation Accuracy: 94.98%\n",
    "  - Test Accuracy: 94.63%\n",
    "\n",
    "- **Epochs: 30**\n",
    "  - Training Accuracy: 95.92%\n",
    "  - Validation Accuracy: 95.74%\n",
    "  - Test Accuracy: 95.47%\n",
    "\n",
    "### Observations:\n",
    "- Increasing the number of epochs led to an improvement in both training and test accuracy, though the rate of improvement slowed down after 20 epochs.\n",
    "- **30 epochs** provided the best balance between training and test accuracy, with a test accuracy of 95.47%. Thus, I decided to use **30 epochs** for further experiments.\n",
    "\n",
    "---\n",
    "\n",
    "## Experiment 2: Modifying Batch Size (Fixed Epochs: 30)\n",
    "\n",
    "After determining that 30 epochs gave the best performance, I experimented with different batch sizes (64, 128, and 256) to see how they impacted the accuracy.\n",
    "\n",
    "### Results:\n",
    "\n",
    "- **Batch Size: 64**\n",
    "  - Training Accuracy: 97.52%\n",
    "  - Validation Accuracy: 96.58%\n",
    "  - Test Accuracy: 96.53%\n",
    "\n",
    "- **Batch Size: 128 (Default)**\n",
    "  - Training Accuracy: 95.92%\n",
    "  - Validation Accuracy: 95.74%\n",
    "  - Test Accuracy: 95.47%\n",
    "\n",
    "- **Batch Size: 256**\n",
    "  - Training Accuracy: 93.84%\n",
    "  - Validation Accuracy: 93.89%\n",
    "  - Test Accuracy: 93.89%\n",
    "\n",
    "### Observations:\n",
    "- **Batch Size 64**: Provided the best accuracy across all datasets (training, validation, and test). The smaller batch size led to more frequent weight updates, which likely contributed to the better learning performance, though it increased training time.\n",
    "- **Batch Size 128**: The default configuration provided good results, but it did not perform as well as Batch Size 64.\n",
    "- **Batch Size 256**: The larger batch size reduced the accuracy overall. While larger batches process more data per step, they lead to less frequent updates, which seems to have limited the model’s ability to learn as effectively.\n",
    "\n",
    "---\n",
    "\n",
    "## Experiment 3: Modifying Hidden Neurons (Fixed Epochs: 30, Batch Size: 64)\n",
    "\n",
    "In this experiment, I varied the number of hidden neurons (64, 128, 256) to see how this parameter affects the model’s performance.\n",
    "\n",
    "### Results:\n",
    "\n",
    "- **Hidden Neurons: 64**\n",
    "  - Training Accuracy: 97.00%\n",
    "  - Validation Accuracy: 96.18%\n",
    "  - Test Accuracy: 96.26%\n",
    "\n",
    "- **Hidden Neurons: 128 (Default)**\n",
    "  - Training Accuracy: 97.52%\n",
    "  - Validation Accuracy: 96.58%\n",
    "  - Test Accuracy: 96.53%\n",
    "\n",
    "- **Hidden Neurons: 256**\n",
    "  - Training Accuracy: 97.93%\n",
    "  - Validation Accuracy: 96.95%\n",
    "  - Test Accuracy: 96.97%\n",
    "\n",
    "### Observations:\n",
    "- **Hidden Neurons 64**: The performance decreased slightly compared to the default, with a test accuracy of 96.26%. The model had fewer parameters to train, which likely reduced its ability to capture complex patterns.\n",
    "- **Hidden Neurons 128 (Default)**: This provided a good balance between model complexity and accuracy, yielding a test accuracy of 96.53%.\n",
    "- **Hidden Neurons 256**: Increasing the number of hidden neurons improved the accuracy further, with the best test accuracy of 96.97%. However, this came at the cost of longer training times.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion:\n",
    "The model performs best with **30 epochs**, **batch size of 64**, and **256 hidden neurons**. This combination yielded the highest test accuracy of **96.97%**. Increasing the number of hidden neurons helped improve the model's capacity to learn complex patterns, while using a smaller batch size allowed for more frequent updates, leading to better performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
